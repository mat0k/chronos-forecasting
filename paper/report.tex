\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{float} % for [H]
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{xurl} % better line breaks for \url and \path
\usepackage{array}
\usepackage{hyperref}
% preamble
\usepackage{caption}
\usepackage{multirow}

\captionsetup[table]{
  font=small,
  labelfont=bf,
  textfont=normalfont
}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\newcommand{\code}[1]{\texttt{\path{#1}}} % allows breaking at _, =, /, etc.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{biblatex} %Imports biblatex package
\addbibresource{references.bib} %Import the bibliography file
\begin{document}

% \title{Chronos-2: A Replication and Extension Study}
\title{Chronos-2: A Replication and Extension Study}

%\author{\IEEEauthorblockN{MAATOUK HASSAN}
%\IEEEauthorblockA{\textit{S343952} \\
%\textit{Politecnico Di Torino}}
%\and
%\IEEEauthorblockN{ALI OWAIS}
%\IEEEauthorblockA{\textit{S338975} \\
%\textit{Politecnico Di Torino}}
%\and
%\IEEEauthorblockN{EL GHAZEL HASSINE}
%\IEEEauthorblockA{\textit{S346265} \\
%\textit{Politecnico Di Torino}}
%\and
%\IEEEauthorblockN{RAHMANI ARASH}
%\IEEEauthorblockA{\textit{S343938} \\
%\textit{Politecnico Di Torino}}
%\and
%\IEEEauthorblockN{Van Den Berk Koen}
%\IEEEauthorblockA{\textit{S350800} \\
%\textit{Politecnico Di Torino \\ 
%Eindhoven University of Technology }}
%}

\author{
Hassan Maatouk$^{1}$, 
Owais Ali$^{1}$, 
Hassine El Ghazel$^{1}$, 
Arash Rahmani$^{1}$, 
Koen Van Den Berk$^{1,2}$\\
\textit{$^{1}$Politecnico Di Torino, $^{2}$Eindhoven University of Technology}\\
\textit{S343952, S338975, S346265, S343938, S350800}
}

\maketitle



\begin{abstract}
Chronos-2 is a state-of-the-art framework for universal time series forecasting. This report evaluates four targeted extensions aimed at improving computational efficiency and forecast accuracy through inference-time modifications.

We evaluate: (1) sparse windowed temporal attention to reduce computational cost, (2) augmented inference with engineered covariates or variables, (3) soft similarity-weighted group masking to enable cross-group learning, and (4) semantic cross-learning through similarity-based context construction. Our experiments on standard benchmarks reveal that sparse attention preserves accuracy while discarding substantial attention mass, though runtime gains are not realized without optimized kernels. Augmented inference degrades performance, likely due to distribution mismatch. Soft masking shows no significant improvement over baseline hard masking, suggesting the model's pretrained representations already capture relevant cross-series dependencies. Semantic cross-learning demonstrates directional improvements on two-thirds of tasks, with statistical significance under non-parametric testing.

\href{https://github.com/orgs/Tokenizers-group/repositories}{Code}
\end{abstract}

 
\section{Introduction}

Time series forecasting is fundamental to decision-making across finance, energy systems, supply chain management, and environmental monitoring. Traditional forecasting methods are typically trained on specific datasets, limiting their ability to generalize across domains. Inspired by the success of foundation models in natural language processing, recent research has pursued universal time series forecasting through large-scale pretrained transformer architectures. Chronos-2 \cite{ansari2025chronos2} represents a state-of-the-art approach in this direction, reformulating forecasting as a sequence modeling problem by discretizing time series values into tokens. This design enables unified treatment of univariate and multivariate settings with probabilistic predictions via quantile estimation, demonstrating strong zero-shot performance across diverse benchmarks. While Chronos-2's pretrained representations are robust, opportunities remain to enhance performance through targeted inference-time modifications that preserve model compatibility and require no retraining. This work investigates whether inference-time interventions targeting attention mechanisms, input construction, and batch composition can meaningfully improve forecasting accuracy or computational efficiency. 




% \section{Methodology}

% Chronos-2 is built on an \textbf{encoder-only transformer} architecture that reformulates time series forecasting as a sequence modeling task. Input time series are discretized into tokens through patching and quantization, enabling transformer-based processing. The architecture employs two key attention mechanisms: \textbf{time attention}, which applies self-attention along the temporal axis using rotary position embeddings (RoPE) to capture dependencies across time, and \textbf{group attention}, which aggregates information across related time series that share the same group ID within a batch. Group attention enables flexible in-context learning scenarios, groups may represent single series (univariate forecasting), multiple related series (cross-learning), multiple variates (multivariate forecasting), or combinations of targets and covariates. This group-based mechanism is central to Chronos-2's universal forecasting capability and is directly relevant to several extensions evaluated in this work.

% During inference, forecasting proceeds autoregressively: future tokens are generated sequentially conditioned on historical context and previously predicted values. The model outputs multiple quantiles at each forecasting step, enabling probabilistic predictions that capture uncertainty. All extensions in this work modify only the inference pipeline, specifically the attention mechanisms (sparse time attention), input construction (augmented inference), group masking logic (soft masking), or batch composition (semantic cross-learning), while using the pretrained Chronos-2 model weights unchanged. We evaluate on standard benchmarks including Chronos Benchmark II and FEV using MASE (Mean Absolute Scaled Error) and WQL (Weighted Quantile Loss) as primary metrics.



%%%%%%%%%%


\section{Problem Statement}

This work investigates four inference-time modifications to the pretrained Chronos-2 forecasting framework. Each extension addresses a specific limitation or opportunity while maintaining compatibility with the pretrained model. We formalize the problem setting for each extension by specifying the expected input, the addressed task, and the expected output. All extensions operate exclusively during inference using the frozen Chronos-2 model, requiring no retraining or architectural modifications.

\subsection{Sparse Time Attention}
Chronos-2 performs dense temporal self-attention over the context window, which scales quadratically with the number of tokens and can become a computational bottleneck on long horizons \cite{vaswani2017attention}. The problem is to determine whether Chronos-2 can retain zero-shot forecast performance when context-to-context temporal attention is sparsified using a local window of radius $r$,\cite{beltagy2020longformer, child2019sparse} while keeping attention for the future-tokens dense and leaving all model weights unchanged. 
% Formally, we apply sparse temporal attention to context tokens while using global attention for the future tokens and evaluate the trade-off between accuracy and computational cost, as well as how much full-attention probability mass is preserved by the sparse pattern. 
% To be completed

\subsection{Augmented Inference}
This extension attempts to improve the pre-trained Chronos-2 model by adding extra information during inference. Inspiration for this extension is recent literature establishing that repeating a prompt in a causal, non-reasoning LLM enhances performance \cite{leviathan2025promptrepetitionimprovesnonreasoning}. Similarly, Chronos-2 could improve if the ``prompt'' is augmented. Chronos-2 is given an original univariate input sequence in a zero-shot context and additional inputs are engineered from the original input. The additional inputs are added as covariates or as variables to be predicted. The output is a univariate prediction plus the corresponding quantiles.

\subsection{Soft Group Masking}

Chronos-2's group attention mechanism uses binary masking: time series with identical group IDs can attend to each other, while series in different groups cannot exchange information. This prevents leakage across unrelated series but may block useful information when related series are assigned to different groups due to batching constraints. The objective is to investigate whether relaxing this hard constraint through similarity-weighted cross-group attention can improve forecast accuracy when correlated time series co-occur in the same batch but belong to different groups. Given a batch of $B$ time series $\{\mathbf{x}_1, \ldots, \mathbf{x}_B\}$ with group IDs, we aim to produce quantile forecasts $\{\hat{\mathbf{y}}_i^{(q)}\}$ that leverage pairwise similarity to modulate cross-group information flow without compromising the model's ability to prevent interference between unrelated series.


\subsection{Semantic Cross-Learning}
Chronos-2 \cite{ansari2025chronos2} supports inference-time information sharing via \emph{cross-learning} (ICL) by assigning multiple series in a batch the same group ID, enabling cross-series context without retraining or changing weights. However, arbitrary batching may yield incoherent helpers, which is problematic on heterogeneous benchmarks like \textsc{FEV} \cite{shchur2025fev} (diverse domains, frequencies, seasonalities, noise, and covariates), making performance sensitive to batch composition. We therefore propose a pre-inference grouping strategy that selects helpers by similarity while keeping Chronos-2 inference code and weights unchanged.
% To be completed
\section{Proposed Extensions}
% The proposed extensions aim to enhance the Chronos-2 framework by addressing specific limitations and exploring new capabilities. These extensions are conceptualized to improve efficiency, generalization, and forecasting quality while maintaining alignment with the original architecture.

\subsection{\textbf{First extension: Sparse Time Attention}}
One extension assesses replacing dense temporal attention mechanisms with sparse attention, while keeping the tokenization, group attention and quantile forecasting pipeline unchanged. Concretely, we implemented a \textbf{windowed} temporal attention scheme for the past history in which each token attends to a local neighborhood of radius $r$ ``local\_radius'', while the future attends to all previous tokens in a dense way. Attention computation is performed in chunks to reduce memory pressure and enable efficient kernels when available. The extension is integrated into the existing pipeline via configuration flags (attention type, backend, radius, chunk size), allowing controlled comparisons against the original full-attention baseline under identical preprocessing and evaluation settings. Sparse attention theoretically reduces computational complexity by focusing on the most relevant time steps, potentially improving efficiency without compromising accuracy\cite{zaheer2020bigbird}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{./images/sparseAttention.png}
    \caption{Sparse time attention: windowed context (radius $r$) and global future attention.}
    \label{fig:sparseAttention}
\end{figure}

To quantify how much attention is discarded by sparsification, we run the pretrained model with full temporal attention and extract per-layer attention weights. For a given radius $r$, we compute the fraction of the attention probability mass that falls inside the sparse mask. This yields a model dependent 'mass retention' curve, complementary to the purely geometric edge-retention fraction. All experiments are performed in a zero-shot setting using the pre-trained Chronos-2 weights. 

% Another extension explores the combination of cross-learning and cross-group learning. By leveraging shared patterns across different time-series groups, this approach aims to enhance generalization and robustness. This is especially relevant for datasets with diverse characteristics, where group-level patterns can complement individual series-level insights.
\subsubsection*{Experimental Setup}
To quantify "how much attention is discarded" by sparse time attention, we run chronos-2 with full time attention and extract the encoder time self-attention weights. For each radius $r$, we construct the sparse mask matching our implementation and compute the fraction of full-attention probability mass that lies inside the allowed pattern (kept mass), restricted to context queries. We also report the purely structural fraction of retained query-key edges (kept edges). Since Chronos-2 uses patching, we additionally stratify results by the effective token length $S$.
We evaluate the pretrained Chronos-2 pipeline on Chronos Benchmark II tasks\cite{ansari2024chronos}. We compare full time attention against sparse time attention with radii $r \in {8,16,32,64,128}$ and compute per-task performance delatas $\triangle MASE = MASE_{sparse} - MASE_{full}$ following the benchmark's build-in evaluation. Statistical significance is assessed using a paired t-test over tasks. Inference time is measured as median over $N$ repeats per task, using CUDA synchronization before and after each run, reporting the task-level inference time and speedup ratio ($t_{sparse}/t_{full}$). We also record peak GPU allocated and researved memory per task (median across repeats). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{./images/sparsevsFullAttention.png}
    \caption{Attention mass retained vs radius (context queries only)}
    \label{fig:sparsevsFullAttention}
\end{figure}
\subsubsection*{Results}
Because patching yields very different effective token lengths $S$, we stratify by $S$. In long-token windows ($S \ge 128$), small radii retain only a small fraction (\aprox 3.3\%) of the full-attention mass for context queries, evident from the Figure~\ref{fig:sparsevsFullAttention}. Across radii, the retained mass closely tracks the retained edge fraction, indicating that aggregated context-query attention is broadly distributed across context keys rather than sharply concentrated locally. 

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{rccc}
\toprule
\textbf{Radius} & \textbf{Median $\Delta$MASE} & \textbf{Median speedup} & \textbf{$p$-value} \\
\midrule
8   & -0.000086 & 0.965846 & 0.107531 \\
16  & -0.000091 & 0.959966 & 0.887393 \\
32  &  0.000009 & 0.960674 & 0.288954 \\
64  & -0.000117 & 0.961477 & 0.221018 \\
128 & -0.000032 & 0.963810 & 0.219545 \\
\bottomrule
\end{tabular}
\caption{Sparse time attention vs.\ full attention (all contexts pooled). $\Delta\text{MASE}$ (negative is better). We report medians to reduce sensitivity to outlier tasks. Speedup values $<1$ indicate sparse is slower).}
\label{tab:sparse_radius_summary_minimal}
\end{table}


Despite discarding most of this mass at small radii, performance remains essentially unchanged, suggesting that the dropped long-range attention mass is not critical for CBII accuracy in this regime.  
Across the evaluated CBII tasks evaluated across different radii, sparse time attention achieves near-pairty with full attention. The average $\triangle MASE (sparse-full)$ across the radii stay close to zero, and paired t-tests over the task-level deltas does not reject the null hypothesis of equal mean performance (all ($p>0.1$). In our current implementation, sparse attention does not yield runtime gains; speedup ratios, computed from median inference time over repeats, are less than 1, indicating overhead dominates under this token regime. 
% Across all LTSF datasets, the MAE/MSE differences between sparse and full attention are extremely small (mostly in order of 1e-5 to 1e-3, with ILI showing slightly larger $\triangle MSE$ but still small relative to scale). This indicates that sparse attention preserves forecast quality under the same inference pipeline. 
% In terms of memory, peak GPU memory increases as context grows (expected), but the difference between sparse and full is negligible. This suggest that for these workloads, sparse attention does not reduce peak GPU footprint measurably. 
% With increading radii under the same benchmark configuration, the speedup remains close to 1.0, thus it does not change the dominant compute path, primarily becuase the model attends over a relatively small number of effective tokens after patching, so sparse overhaed dominates. 
\subsubsection*{Results Analysis}
Chronos-2's architectural step of patching reduces the practical benefit of sparsifying the attention matrix\cite{nie2022patchtst}. Secondly, although aggregated full-attention weights over context queries appear broadly distributed, CBII accuracy is largely insensitive to removing long-range context links, suggesting redundancy in how historical information is repeated (e.g. via patching, residuals and summary tokens). Consequently, sparse attention can match full-attention accuracy but does not improve runtime without a more optimized sparse attention kernal\cite{dao2023flashattention2}. 
% \subsection{Outcome}
% Our sparse attention variant preserves zero-shot CBII accuracy relative to full attention while discarding a substantial portion of context-query attention mass at long token lengths. However, we do not observe speed or memory improvements in our current implementation. This is consisnt with Chronos-2's patch based tokenization (small effective $S$) and the lack of highly optimized sparse attention kernal in this setting.





\subsection{\textbf{Second extension: Augmented inference}}
The extension consists of two related but different approaches. The first approach involves injecting transformed versions of an input time series $\mathbf{x}$ as covariates. Specifically, let 
\begin{alignat}{3}
    f_1(\mathbf{x}) &= \mathbf{x}^3, & \; \; f_2(\mathbf{x}) &= \exp \mathbf{x},& \; \; f_3(\mathbf{x})&=-\mathbf{x}, \\ f_4(\mathbf{x})&=-\mathbf{x}^3, &f_5(\mathbf{x})&=-\exp \mathbf{x},
\end{alignat}
where operations are element-wise. 
To ensure significant transformation and prevent impractically large values, we do not transform $\mathbf{x}$ directly, but transform a normalized version. So, the covariates introduced are $$f_i\left(\frac{\mathbf{x}-\mu(\mathbf{x})}{\sigma(\mathbf{x})}\right), \; i=1,2,3,4,5.$$

The idea is that these covariates could help Chronos-2 recognize patterns that would otherwise remain unnoticed. A tempting strategy, given the prompt-repetition result for LLMS is to simply include $\mathbf{x}$ as a covariate. However, by construction of Chronos-2, attention to the original time series is equivalent to attention to the covariate. Hence, adding $\mathbf{x}$ as a covariate has zero effect. In addition to the $f_i$'s, an experiment is also conducted with covariates
\begin{equation}
    g_1(\mathbf{x})=\text{MovingAverage}(\mathbf{x},9),\quad g_2(\mathbf{x})=\mathbf{x}-g_1(\mathbf{x}).
\end{equation}
So Chronos-2 is given a smoothed version of $\mathbf{x}$ and the residual of the smoothed version. A sliding window of 9 was chosen based on the patch size of 16. Edges are handled by repeating the outer values as padding.

The second approach involves considering these covariates as time series to be predicted. These additional predictions are then combined with the original prediction. Observe that $f_i$ are invertible functions. Under the assumption that Chronos-2 is truly a zero-shot model, there is no reason to expect the prediction $\text{Chronos-2}(\mathbf{x})$ to be better than $f^{-1}(\text{Chronos-2}(f(\mathbf{x})).$ After all, if Chronos-2 made perfect predictions, both vectors would be equal.
In practice, the performance of {Chronos-2} may be biased due to the nature of the training data. Therefore, combining the predictions $f_i^{-1}(\text{Chronos-2}(f_i(\mathbf{x}))$ could be more robust. The final prediction is the unweighted average over all predictions (original and transformed). The same is done for quantiles. Some important notes on this approach:
\begin{itemize}
    \item Input is normalized before transformation. Hence, predictions need to be normalized. Precisely, the additional predictions for $i=1,\dots,5$. 
    \begin{equation}
        \mu(\mathbf{x}) + \sigma(\mathbf{x}) f_i^{-1}\left(\text{Chronos-2}\left(f\left(\frac{\mathbf{x}-\mu(\mathbf{x})}{\sigma(\mathbf{x})}\right)\right)\right).
    \end{equation}
    \item Functions $f_2$ and $f_5$ are only invertible if the prediction is positive. A perfect predictor would always produce such values. An ad-hoc solution is to clip small or negative values to some small constant $\varepsilon$.
    \item Decreasing transformation functions, like $f_3,f_4$ and $f_5$, will yield predicted quantiles in flipped order. So a predicted $q$-quantile will correspond to a $(1-q)$-quantile of $\mathbf{x}$.
\end{itemize}
\subsection*{Experimental Setup}
The experiment targets zero-shot performance on univariate datasets. All four variants are evaluated on the Chronos-Zeroshot collection. Each variant is applied to both Chronos-2 and Chronos-2-Synth (Chronos-2 trained only on synthetic data). This gives a total of 8 experiments. The win rate, MASE and WQL is analyzed in each experiment. These are then compared with the baseline, being Chronos-2 and Chronos-2-Synth, respectively. A statistically significant improvement in any of the three metrics is considered promising.
\vspace{-.3em}
\subsection*{Results}
None of the four experiments produce a promising result: the win rate is always lower with respect to the baseline. Mean MASE and WQL is also lower for each experiment. Inference time is 6 times as high when using $f_i$'s and 3 times as high for when using $g_i$'s. As expected, the inference time is directly proportional to the number of input sequences. The full table can be found in \autoref{tab:aug}.

% Required in preamble: \usepackage{booktabs, multirow, array}
% Required: \usepackage{booktabs, multirow, array}

% Required: \usepackage{booktabs, multirow, array}

% Required in preamble: \usepackage{booktabs, multirow, array}

\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{5pt} 
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{>{\raggedright\arraybackslash}p{1.2cm} l c c c >{\centering\arraybackslash}p{1.2cm}}
\toprule
\textbf{Base model} & \textbf{Variant} & \textbf{Win rate} & \textbf{MASE} & \textbf{WQL} & \textbf{Inference time (s)} \\
\midrule
\multirow{5}{=}{Chronos-2}       & Baseline           & 0.76 & 1.37 & 0.14 & 21  \\
                                 & Covariates $f_i$   & 0.51 & 1.44 & 0.14 & 118 \\
                                 & Covariates $g_i$   & 0.58 & 1.42 & 0.15 & 60  \\
                                 & Copredict $f_i$    & 0.31 & 1.60 & 0.17 & 123 \\
                                 & Copredict $g_i$    & 0.44 & 1.45 & 0.15 & 63  \\
\midrule
\multirow{5}{=}{Chronos-2-Synth} & Baseline           & 0.69 & 1.41 & 0.14 & 21  \\
                                 & Covariates $f_i$   & 0.55 & 1.43 & 0.14 & 119 \\
                                 & Covariates $g_i$   & 0.62 & 1.41 & 0.14 & 61  \\
                                 & Copredict $f_i$    & 0.41 & 1.66 & 0.19 & 120 \\
                                 & Copredict $g_i$    & 0.48 & 1.45 & 0.15 & 61  \\
\bottomrule
\end{tabular}
\caption{Win rate, mean MASE, mean WQL and mean inference time for variants of the augmented inference extension. Evaluated on Chronos-Zeroshot benchmark}
\label{tab:aug}
\end{table}

\subsection*{Interpretation}
Augmented inference does not result in out-of-the-box improvement of Chronos-2. In fact, performance declines. Further research may explore first fine-tuning using augmented inputs. Note that adding inputs increases inference time. As a result, augmented inference may not be an attractive candidate for further experimentation.

\subsection{\textbf{Third extension: Soft Group Masking Extension}}


\paragraph{Motivation and Hypothesis}
Chronos-2 employs hard group masking to enable batch-level cross-learning, where time series with identical group IDs can attend to each other while series in different groups cannot exchange information \cite{ansari2025chronos2}. This binary attention constraint is expressed as a hard mask $\mathbf{M}_{\text{hard}} \in \{0, 1\}^{B \times B}$ where element $[i,j]$ equals 1 if time series $i$ and $j$ belong to the same group, and 0 otherwise. While this prevents information leakage across unrelated series, it may inadvertently block useful information exchange when related time series are assigned to different groups due to random batching. \textbf{We hypothesize} that replacing hard masking with similarity-weighted soft masking at inference time will improve forecast accuracy when related series are co-located in the same batch but assigned to different groups, particularly for datasets with correlated time series.

\paragraph{Technical Approach}
Our extension replaces the binary hard mask with a continuous soft mask $\mathbf{M}_{\text{soft}} \in [0, 1]^{B \times B}$ that encodes pairwise similarity between time series:
\begin{equation}
\mathbf{M}_{\text{soft}}[i,j] = \begin{cases}
1.0 & \text{if } \text{group\_id}[i] = \text{group\_id}[j] \\
\text{sim}(\mathbf{x}_i, \mathbf{x}_j) & \text{otherwise}
\end{cases}
\end{equation}
where $\text{sim}(\cdot, \cdot): \mathbb{R}^T \times \mathbb{R}^T \rightarrow [0, 1]$ measures similarity between input context sequences. This soft mask is integrated into Chronos-2's self-attention via an additive attention bias:
\begin{equation}
\text{attention\_bias}[i,j] = \log(\mathbf{M}_{\text{soft}}[i,j]) \times \tau
\end{equation}
where $\tau = 5.0$ is a temperature parameter balancing cross-group information flow and training distribution fidelity. The logarithm ensures zero similarity results in $-\infty$ bias (complete masking), while high similarity approaches zero bias (full attention).

We employ Pearson correlation to measure linear relationships between standardized time series $\hat{\mathbf{x}} = (\mathbf{x} - \mu)/\sigma$:
\begin{equation}
\text{sim}_{\text{corr}}(\mathbf{x}_i, \mathbf{x}_j) = \frac{1}{2}\left(1 + \frac{\sum_{t=1}^T (\hat{x}_{i,t})(\hat{x}_{j,t})}{\sqrt{\sum_{t=1}^T \hat{x}_{i,t}^2 \sum_{t=1}^T \hat{x}_{j,t}^2}}\right)
\end{equation}
mapping correlation from $[-1, 1]$ to $[0, 1]$.

\paragraph{Implementation Details}
The soft masking extension is implemented with minimal modifications to the Chronos-2 codebase. We add a similarity computation module that calculates pairwise relationships between input time series based on their raw context values. Critically, this is an \textbf{inference-only} approach requiring no model retraining. Users can enable soft masking through optional parameters in the prediction interface, allowing seamless switching between hard and soft masking modes while maintaining full backward compatibility.

\paragraph{Results}
We evaluated soft group masking against baseline hard masking across the Chronos Benchmark II datasets using MASE (Mean Absolute Scaled Error) and WQL (Weighted Quantile Loss). Table~\ref{tab:soft-masking-results} summarizes the aggregate performance.

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Soft Masking} & \textbf{Improvement} \\
\midrule
MASE & 2.7686 & 2.6811 & +2.8\% \\
WQL  & 0.1514 & 0.1505 & +0.39\% \\
\bottomrule
\end{tabular}
\caption{Baseline (hard masking) vs. soft masking performance on Chronos Benchmark II. Lower is better for both metrics.}
\label{tab:soft-masking-results}
\end{table}

While soft masking shows modest numerical improvements, paired t-tests reveal these differences are not statistically significant (MASE: $p=0.211$; WQL: $p=0.765$). We conclude that the \textbf{hypothesis is not supported}: soft masking does not provide meaningful improvements over standard hard masking in Chronos-2.

To further investigate whether correlation structure influences the effectiveness of soft masking, we stratified datasets by their expected cross-series dependencies. For correlated datasets (ETTh1, Weather, where sensors or temporal patterns exhibit strong dependencies), soft masking showed a stronger directional trend toward significance (paired t-test: $p=0.124$) compared to weakly correlated datasets such as Walmart time series ($p=0.266$). While both remain above the significance threshold, the directional pattern suggests that soft masking may provide marginal benefits in highly correlated settings, though the effect is too small to be reliable under current inference-only application.

We attribute this null result to three factors: (1) \textbf{training-inference mismatch}—the model was trained exclusively with hard masking, creating distribution shift when soft masking is applied only at inference; (2) \textbf{sufficiency of learned representations}—Chronos-2's pretraining likely already captured relevant cross-series dependencies through group attention, leaving limited room for similarity-based refinement; and (3) \textbf{similarity metric limitations}—raw Pearson correlation may not align with the latent similarity structures the model internally relies upon for cross-learning.



\subsection{\textbf{Fourth extension: Context Construction and Batched Inference}}
\label{sec:chronos2-extension}
Chronos-2 \cite{ansari2025chronos2} controls inference-time information sharing via \emph{group IDs} within each batch. With cross-learning \emph{off}, each series gets its own group ID, yielding standard univariate inference with independent predictions. With cross-learning \emph{on}, multiple items can share a group ID so the model can use shared context during the (unchanged) forward pass. Our extension modifies only \emph{batch composition}: for each target series, we optionally attach a small set of auxiliary \emph{helper} series and assign them the same group ID, creating an \textbf{augmented cross-learning context} while keeping Chronos-2 tokenization, parameters, and inference path unchanged. We evaluate three modes differing only in group formation: \textbf{Baseline} (cross-learning off; no helpers), \textbf{Random cross-learning} (helpers sampled uniformly at random), and \textbf{Semantic cross-learning} (helpers retrieved by cosine similarity between lightweight \emph{series signatures} computed from history windows, then packed with a deterministic policy: Top-$K$ first, weak fill, and a global fallback to reduce fragmentation). In all cases, only grouping and batching change; the Chronos-2 forward pass is identical.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{images/chronos2-extension.png}
  \caption{Chronos-2 inference pipeline with the proposed extension layer. The Chronos-2 forward pass is unchanged; the extension operates strictly \emph{before} inference by constructing grouped contexts and assembling batches.}
  \label{fig:chronos2-extension}
\end{figure}
\subsubsection{\textbf{Experiments and evaluations}} Experiments are run under a single, fixed evaluation configuration; Table~\ref{tab:chronos2-ext-setup} summarizes the benchmarks, metrics, shared inference settings, and the semantic extension defaults used in our implementation. Unless otherwise stated, only \emph{grouping and batch construction} change; the Chronos-2 model forward pass remains identical across modes.

\begin{table}[H]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\columnwidth}{@{}p{0.50\columnwidth}Y@{}}
\hline
\textbf{Parameter / setting} & \textbf{Value} \\
\hline
Benchmarks & \code{fev-bench} (\textit{80 tasks over 100 successfully tested}) \\
Metrics & \code{MASE}, \code{WQL} \\
Modes compared &
Baseline (\emph{cross-learning off}); Random cross-learning (Chronos-2 ICL); Semantic cross-learning (ours) \\
Model (\code{model\_id}) & \code{amazon/chronos-2} \\
Precision (\code{dtype}) & \code{float32} \\
Batch size & \code{32} \\
Semantic grouping & \code{neighbors} \\
Top-$K$ & \code{64} \\
Neighbor threshold & \code{0.20} \\
Alt. clustering params (if used) & \code{num\_clusters=50}, \code{kmeans\_iters=25} \\
\hline
\end{tabularx}
\caption{Evaluation configuration and semantic extension defaults.}
\label{tab:chronos2-ext-setup}
\end{table}
\label{sec:univar-results}

% Requires: \usepackage{booktabs,tabularx,amsmath}
We compare three inference-time modes on \textsc{FEV} \cite{shchur2025fev} tasks:
\emph{Baseline} (cross-learning off), \emph{Random} cross-learning, and \emph{Semantic} cross-learning.
For \textsc{FEV}, \textbf{we \emph{flatten} each task into a set of univariate forecasting instances: regardless of whether a task is originally univariate, covariate, or multivariate}, evaluation is performed by forecasting a \emph{single target series} per instance. Lower is better for both metrics (\textbf{MASE}, \textbf{WQL}). All Random vs Semantic statistics are \emph{paired across tasks}. We report mean (median) metric values, the mean percent gain of Semantic over Random,
and the fraction of tasks where Semantic strictly outperforms Random. Significance is assessed with a \textbf{paired $t$-test (two-sided)} and a \textbf{Wilcoxon signed-rank test} (one-sided, $\mathrm{H_1}$: Semantic $<$ Random).
% Requires:
% \usepackage{booktabs}
\begin{table}[H]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabular}{@{}l c c c c c c@{}}
\toprule
\textbf{Metric} & \textbf{$n$} &
\textbf{Base} & \textbf{Rand} & \textbf{Sem} &
\textbf{$\Delta(\%)$} & \textbf{$p_t/p_W$} \\
& & \textbf{(mean/med)} & \textbf{(mean/med)} & \textbf{(mean/med)} &
\textbf{\& WR$_{S<R}$} & \\
\midrule
MASE & 80 &
\shortstack{1.373\\(0.834)} &
\shortstack{1.335\\(0.791)} &
\shortstack{\textbf{1.325}\\(0.795)} &
\shortstack{+0.78\%\\WR=66.2\%} &
\shortstack{0.137\\0.0007} \\
\addlinespace[2pt]
WQL & 80 &
\shortstack{0.1789\\(0.1149)} &
\shortstack{0.1744\\(0.1138)} &
\shortstack{\textbf{0.1738}\\(0.1132)} &
\shortstack{+0.69\%\\WR=67.5\%} &
\shortstack{0.201\\0.0029} \\
\bottomrule
\end{tabular}

\caption{Mean/median (lower is better). $\Delta(\%)$: mean percent gain (Semantic vs Random); WR$_{S<R}$: fraction where Semantic beats Random. $p_t$: paired $t$-test (two-sided); $p_W$: Wilcoxon (one-sided, $H_1$: Sem$<$Rand). ``Exploded'' MASE tasks (MASE$>10$ in any mode) are removed, yielding $n{=}80$.}
\label{tab:fev_one_table}
\end{table}

\subsubsection{\textbf{Interpretation}}
Semantic cross-learning shows a consistent \emph{directional} advantage over Random Table~\ref{tab:fev_one_table}: it wins on about two-thirds of tasks (WR$_{S<R}\approx 66$--$68\%$) for both \textbf{MASE} and \textbf{WQL}. This is reflected by the Wilcoxon signed-rank test, which is significant under standard thresholds ($p_W<0.05$ for both metrics), indicating the improvements are systematic across tasks. In contrast, the paired $t$-test is not significant ($p_t>0.05$) because the \emph{mean} gains are small and task-level differences are heavy-tailed, so a few large deviations can mask many small wins in the average. When retrieved neighbors closely match the target dynamics, gains can be substantial, e.g., \texttt{bizitobs\_l2c\_1H} (\textbf{+7.69\%} MASE, \textbf{+5.19\%} WQL), \texttt{world\_tourism} (\textbf{+1.65\%} MASE, \textbf{+4.77\%} WQL), \texttt{m5\_1W} (\textbf{+10.63\%} MASE, \textbf{+8.55\%} WQL), and \texttt{rohlik\_orders\_1W} (\textbf{+2.31\%} MASE, \textbf{+10.19\%} WQL). Degradations occur on a minority of datasets (e.g., \texttt{walmart}: \textbf{-7.24\%} MASE, \textbf{-2.93\%} WQL; \texttt{rohlik\_orders\_1D}: \textbf{-0.55\%} MASE, \textbf{-5.21\%} WQL), consistent with occasional neighbor mismatch introducing incompatible structure into the shared context.

Finally, statistics are computed on the tasks that completed successfully: after excluding ``exploded'' MASE cases (MASE$>10$ in any mode) and removing 17/100 tasks that could not be evaluated due to limited computational capabilities, paired comparisons use $n{=}80$ tasks.





% \section{Experiments}

% \subsection{Datasets}
% The experimental evaluation leverages two primary sources of data: real-world datasets and synthetic datasets. 

% The real-world datasets follow those used in the original Chronos-2 study, covering diverse domains such as energy, transportation, natural phenomena, and cloud operations. These include benchmark datasets like M4 (Daily, Hourly, Monthly, Weekly), Electricity, Solar, Taxi, and others. Their varied frequencies and temporal characteristics provide a robust foundation for assessing the model's generalization capabilities across different tasks.

% In addition to real-world univariate data, we created a \textbf{custom multivariatizer tool} to generate synthetic multivariate time series from base univariate generators. The multivariatizer introduces realistic dependencies across multiple dimensions by combining, transforming, and synchronizing base series. This approach addresses the scarcity of multivariate training data and allows the model to learn generalizable patterns for univariate, multivariate, and covariate-informed forecasting.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{./images/multi.jpeg}
%     \caption{Correlation and structure of time series before and after applying the multivariatizer.}
%     \label{fig:multivariatizer}
% \end{figure}

% Figure~\ref{fig:multivariatizer} illustrates the impact of the multivariatizer. The left panels show example time series before (top) and after (bottom) processing: initially independent series with only seasonal alignment become synchronized and exhibit structured dependencies. The right panels display correlation matrices, highlighting a clear increase in inter-series correlations after applying the multivariatizer, demonstrating its ability to introduce meaningful relationships between series.

% Together, these datasets ensure comprehensive coverage of scenarios, facilitating a thorough evaluation of the Chronos-2 framework and its proposed extensions.


% \subsection{Experimental Setup}
% \subsubsection{Sparse Attention Extension}
% To quantify "how much attention is discarded" by sparse time attention, we run chronos-2 with full time attention and extract the encoder time self-attention weights. For each radius $r$, we construct the sparse mask matching our implementation and compute the fraction of full-attention probability mass that lies inside the allowed pattern (kept mass), restricted to context queries. We also report the purely structural fraction of retained query-key edges (kept edges). Since Chronos-2 uses patching, we additionally stratify results by the effective token length $S$.
% We evaluate the pretrained Chronos-2 pipeline on Chronos Benchmark II tasks. We compare full time attention against sparse time attention with radii $r \in {8,16,32,64,128}$ and compute per-task performance delatas $\triangle MASE = MASE{sparse} - MASE{full}$. Statistical significance is assessed using a paired t-test over tasks. Inference time is measured as median over $N$ repeats per task, using CUDA synchronization before and after each run and we also record peak GPU allocated and researved memory per task (median across repeats). 

% \subsection{Evaluation Metrics}
% \subsubsection{Sparse Attention Extension}
% For accuracy, the CBII tasks are based on MASE, following the benchmark's build-in evaluation. We report task-level inference time and the speedup ratio ($t{full}/t{sparse}$). For the attention retention, we report both the kept attention mass fraction under sparse mask and kept edge fraction, restricted to context queries.  
% \subsubsection{Augmented input} 
% Accuracy metrics
% Metrics include RMSE, MAE, and MAPE.
% Efficiency metrics

% \subsection{Execution Times}
% % Commenting out the below becuase I say reporting execution time is better done in the result section. 
% % \subsubsection{CBII total inference time (full vs sparse)}
% % Sparse was consistently slightly slower (speed <= 1), e.g. ETTh1 speedup $\approx 0.93$, ETTm1 $\approx 0.93$, Weather $\approx 0.93$, while ECL/Traffic were closer to parity ($\approx 0.98$). During the CBII context sweep with ETTh, ETTm and ercot, sparse remained slower for most contexts: average speedup ranged roughly 0.91-0.98, approaching parity at context 8192 (the highest context supported by chronos-2).  
% % Reported execution times for the experiments
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{./images/sparsevsFullAttention.png}
%     \caption{Attention mass retained vs radius (context queries only)}
%     \label{fig:sparsevsFullAttention}
% \end{figure}
% \subsection{Results}
% \subsubsection{Accuracy, Speedup, and Memory(full vs sparse)}
% Because patching yields very different effective token lengths $S$, we stratify by $S$. In long-token windows ($S \ge 128$), small radii retain only a small fraction (\aprox 3.3\%) of the full-attention mass for context queries, evident from the Figure~\ref{sparsevsFullAttention}. Across radii, the retained mass closely tracks the retained edge fraction, indicating that aggregated context-query attention is broadly distributed across context keys rather than sharply concentrated locally. Despite discarding most of this mass at small radii, performance remains essentially unchanged, suggesting that the dropped long-range attention mass is not critical for CBII accuracy in this regime.  
% Across the evaluated CBII tasks evaluated at minimum radius $r=8$, sparse time attention achieves near-pairty with full attention. The average $\triange MASE (sparse-full)$ across the radii stay close to zero, and paired t-tests over the task-level deltas does not reject the null hypothesis of equal mean performance (all ($p>0.1$). In our current implementation, sparse attention does not yield runtime gains; speedup ratios, computed from median inference time over repeats, are close to \aprox 0.973 - 0.973, indicating overhead dominates under this token regime. 
% Across all LTSF datasets, the MAE/MSE differences between sparse and full attention are extremely small (mostly in order of 1e-5 to 1e-3, with ILI showing slightly larger $\triangle MSE$ but still small relative to scale). This indicates that sparse attention preserves forecast quality under the same inference pipeline. 
% In terms of memory, peak GPU memory increases as context grows (expected), but the difference between sparse and full is negligible. This suggest that for these workloads, sparse attention does not reduce peak GPU footprint measurably. 
% With increading radii under the same benchmark configuration, the speedup remains close to 1.0, thus it does not change the dominant compute path, primarily becuase the model attends over a relatively small number of effective tokens after patching, so sparse overhaed dominates. 
% Tables and plots summarizing results
%    a. Tables, plots, etc.
% (To be added after experiments)
% \subsection{Results Analysis}
% Discussion of improvements, degradations, and observed behavior
% (To be detailed later)
% \subsubsection{Sparse Attention vsv Full Attention in accuracy and cost}
% Chronos-2's architectural step of patching reduces the practical benefit of sparsifying the attention matrix. Secondly, although aggregated full-attention weights over context queries appear broadly distributed, CBII accuracy is largely insensitive to removing long-range context links, suggesting redundancy in how historical information is repeated (e.g. via patching, residuals and summary tokens). Consequently, sparse attention can match full-attention accuracy but does not improve runtime without a more optimized sparse attention kernal. 

% \subsubsection{Augmented inference}



%\section{Outcome and Discussion}
% Our sparse attention variant preserves zero-shot CBII accuracy relative to full attention while discarding a substantial portion of context-query attention mass at long token lengths. However, we do not observe speed or memory improvements in our current implementation. This is consisnt with Chronos-2's patch based tokenization (small effective $S$) and the lack of highly optimized sparse attention kernal in this setting. 

%Augmented inference does not result in out-of-the-box improvement of Chronos-2. In fact, performance declines. Further research may explore first fine-tuning using augmented inputs. Note that adding inputs increases inference time. As a result, augmented inference may not be an attractive candidate for further experimentation.

% The sparse temporal attention variant preserves forcasting accuracy relative to the full-attention baseline across the evaluated LTSF datasets. THe observed differences in MAE/MSE between sparse and full are extremely small, suggesting restricting temporal attention to a local window does not meaningfully degrade the forcasting quality under our evaluation pipeline, however, sparse attention did not produce the expected inference-time and memory usage improvement in our experiments. Across both LTSF evaluation and CBII sweeps, sparse attention was slightly slower than full attention, with performance generally close to parity. We attribute this to the Chronos-2's patch-based tokenization: long timestamps contexts map to small number of effective attention tokens, so dense attention remains efficient, and overhead of sparse masking/chunking dominates. 
% Comparison between replicated and original results
% Impact of sparse attention
% Impact of quantile modeling improvements
% Observed trade-offs



% \section{Conclusion and Takeaways}
% % Key outcomes
% Chronos-2 demonstrates scalability and adaptability to diverse datasets.
% % Main challenges during replication and extension
% Challenges include handling missing data and optimizing sparse attention mechanisms.
% % Lessons learned
% The importance of modular design and rigorous validation is emphasized.
% % Possible future work

\printbibliography

\end{document}