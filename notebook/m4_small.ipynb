{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronos-2 Soft Group Masking Test\n",
    "\n",
    "This notebook tests the soft group masking extension for Chronos-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 0: Install modified Chronos from GitHub and dependencies\nimport sys\nimport os\n\n# Check if running in Google Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    # Install from GitHub repository with soft masking extension\n    if not os.path.exists('/content/chronos-forecasting'):\n        !git clone https://github.com/mat0k/chronos-forecasting.git /content/chronos-forecasting\n        !cd /content/chronos-forecasting && git checkout soft_attention_2\n    !pip install -e /content/chronos-forecasting\n    print(\"✓ Installed Chronos from GitHub (soft_attention_2 branch)\")\nelse:\n    print(\"Not in Colab - assuming local modified version is available\")\n    print(\"Make sure you're using the modified Chronos from the soft_attention_2 branch\")\n\n# Install required dependencies\n!pip install -q pandas scipy matplotlib\n\nprint(\"✓ All dependencies installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Import libraries and define helper functions\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom chronos import BaseChronosPipeline\nfrom scipy.stats import ttest_rel\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Helper functions for metric computation\ndef compute_mae(y_true, y_pred):\n    \"\"\"Compute Mean Absolute Error\"\"\"\n    return np.mean(np.abs(y_true - y_pred))\n\ndef compute_rmse(y_true, y_pred):\n    \"\"\"Compute Root Mean Squared Error\"\"\"\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n\nprint(\"✓ Libraries imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Load Chronos-2 model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load Chronos-2 model (uses the modified chronos2 code)\npipeline = BaseChronosPipeline.from_pretrained(\n    \"amazon/chronos-2\",\n    device_map=device,\n)\n\nprint(f\"✓ Loaded Chronos-2 model on {device}\")\nprint(f\"Pipeline type: {type(pipeline).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load M4 Hourly dataset\nimport pandas as pd\n\n# Load M4 Hourly dataset from AutoGluon\nprint(\"Loading M4 Hourly dataset...\")\ndf = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\n\n# Use first 50 series for testing\nn_series = 50\nunique_ids = df['item_id'].unique()[:n_series]\ndf_subset = df[df['item_id'].isin(unique_ids)]\n\n# Split into train and test (use last 48 points as test)\nprediction_length = 48\n\ntrain_data = []\ntest_data = []\n\nfor item_id in unique_ids:\n    series = df_subset[df_subset['item_id'] == item_id]['target'].values\n    train_data.append(series[:-prediction_length])\n    test_data.append(series[-prediction_length:])\n\nprint(f\"✓ Loaded {n_series} time series from M4 Hourly dataset\")\nprint(f\"Context length: {len(train_data[0])}, Prediction length: {prediction_length}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare data for prediction\n",
    "# Extract context (historical data) and future values (ground truth)\n",
    "context = []\n",
    "future_values = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    context.append(torch.tensor(train_data[i], dtype=torch.float32))\n",
    "    future_values.append(torch.tensor(test_data[i], dtype=torch.float32))\n",
    "\n",
    "print(f\"✓ Prepared {len(context)} series for prediction\")\n",
    "print(f\"Prediction length: {prediction_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Run baseline prediction (original hard masking) + compute metrics\nprint(\"Running baseline prediction (hard group masking)...\")\n\n# Use predict method with inputs parameter\nbaseline_forecast = pipeline.predict(\n    inputs=context,\n    prediction_length=prediction_length,\n)\n\n# Extract median forecast\nbaseline_preds = [forecast[0].median(dim=0).values.numpy() for forecast in baseline_forecast]\n\nprint(\"✓ Baseline prediction completed\\n\")\n\n# Compute metrics for baseline\nbaseline_mae_per_series = []\nbaseline_rmse_per_series = []\n\nfor i in range(len(future_values)):\n    y_true = future_values[i].numpy()\n    baseline_mae_per_series.append(compute_mae(y_true, baseline_preds[i]))\n    baseline_rmse_per_series.append(compute_rmse(y_true, baseline_preds[i]))\n\nbaseline_mae_per_series = np.array(baseline_mae_per_series)\nbaseline_rmse_per_series = np.array(baseline_rmse_per_series)\n\n# Display baseline results\nprint(\"=\"*60)\nprint(\"BASELINE RESULTS (Hard Group Masking)\")\nprint(\"=\"*60)\nprint(f\"Average MAE:  {baseline_mae_per_series.mean():.4f} (±{baseline_mae_per_series.std():.4f})\")\nprint(f\"Average RMSE: {baseline_rmse_per_series.mean():.4f} (±{baseline_rmse_per_series.std():.4f})\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Run soft masking prediction + compute metrics\nprint(\"Running soft masking prediction (correlation-based)...\")\n\n# Use predict method with soft masking parameters\nsoft_forecast = pipeline.predict(\n    inputs=context,\n    prediction_length=prediction_length,\n    use_soft_group_mask=True,  # Enable soft masking\n    similarity_type=\"correlation\",  # Use correlation-based similarity\n    soft_mask_temperature=5.0,  # Temperature parameter\n)\n\n# Extract median forecast\nsoft_preds = [forecast[0].median(dim=0).values.numpy() for forecast in soft_forecast]\n\nprint(\"✓ Soft masking prediction completed\\n\")\n\n# Compute metrics for soft masking\nsoft_mae_per_series = []\nsoft_rmse_per_series = []\n\nfor i in range(len(future_values)):\n    y_true = future_values[i].numpy()\n    soft_mae_per_series.append(compute_mae(y_true, soft_preds[i]))\n    soft_rmse_per_series.append(compute_rmse(y_true, soft_preds[i]))\n\nsoft_mae_per_series = np.array(soft_mae_per_series)\nsoft_rmse_per_series = np.array(soft_rmse_per_series)\n\n# Display soft masking results\nprint(\"=\"*60)\nprint(\"SOFT MASKING RESULTS (Correlation-based)\")\nprint(\"=\"*60)\nprint(f\"Average MAE:  {soft_mae_per_series.mean():.4f} (±{soft_mae_per_series.std():.4f})\")\nprint(f\"Average RMSE: {soft_rmse_per_series.mean():.4f} (±{soft_rmse_per_series.std():.4f})\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Statistical comparison between baseline and soft masking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: BASELINE vs SOFT MASKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate improvement percentage\n",
    "mae_improvement = ((baseline_mae_per_series.mean() - soft_mae_per_series.mean()) / baseline_mae_per_series.mean()) * 100\n",
    "rmse_improvement = ((baseline_rmse_per_series.mean() - soft_rmse_per_series.mean()) / baseline_rmse_per_series.mean()) * 100\n",
    "\n",
    "print(f\"MAE Improvement:  {mae_improvement:+.2f}%\")\n",
    "print(f\"RMSE Improvement: {rmse_improvement:+.2f}%\")\n",
    "\n",
    "# Paired t-test for statistical significance\n",
    "mae_t_stat, mae_p_value = ttest_rel(baseline_mae_per_series, soft_mae_per_series)\n",
    "rmse_t_stat, rmse_p_value = ttest_rel(baseline_rmse_per_series, soft_rmse_per_series)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SIGNIFICANCE (Paired t-test)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  t={mae_t_stat:.4f}, p={mae_p_value:.4f}\")\n",
    "print(f\"RMSE: t={rmse_t_stat:.4f}, p={rmse_p_value:.4f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if mae_p_value < alpha:\n",
    "    print(f\"\\n✓ MAE difference is statistically significant (p < {alpha})\")\n",
    "else:\n",
    "    print(f\"\\n✗ MAE difference is NOT statistically significant (p >= {alpha})\")\n",
    "\n",
    "if rmse_p_value < alpha:\n",
    "    print(f\"✓ RMSE difference is statistically significant (p < {alpha})\")\n",
    "else:\n",
    "    print(f\"✗ RMSE difference is NOT statistically significant (p >= {alpha})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 0].hist(baseline_mae_per_series, bins=20, alpha=0.7, label='Baseline', color='blue')\n",
    "axes[0, 0].hist(soft_mae_per_series, bins=20, alpha=0.7, label='Soft Masking', color='orange')\n",
    "axes[0, 0].set_xlabel('MAE')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('MAE Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].hist(baseline_rmse_per_series, bins=20, alpha=0.7, label='Baseline', color='blue')\n",
    "axes[0, 1].hist(soft_rmse_per_series, bins=20, alpha=0.7, label='Soft Masking', color='orange')\n",
    "axes[0, 1].set_xlabel('RMSE')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('RMSE Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Per-series MAE comparison\n",
    "axes[1, 0].scatter(baseline_mae_per_series, soft_mae_per_series, alpha=0.6)\n",
    "axes[1, 0].plot([baseline_mae_per_series.min(), baseline_mae_per_series.max()],\n",
    "                [baseline_mae_per_series.min(), baseline_mae_per_series.max()],\n",
    "                'r--', linewidth=2, label='y=x')\n",
    "axes[1, 0].set_xlabel('Baseline MAE')\n",
    "axes[1, 0].set_ylabel('Soft Masking MAE')\n",
    "axes[1, 0].set_title('Per-Series MAE Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Per-series RMSE comparison\n",
    "axes[1, 1].scatter(baseline_rmse_per_series, soft_rmse_per_series, alpha=0.6)\n",
    "axes[1, 1].plot([baseline_rmse_per_series.min(), baseline_rmse_per_series.max()],\n",
    "                [baseline_rmse_per_series.min(), baseline_rmse_per_series.max()],\n",
    "                'r--', linewidth=2, label='y=x')\n",
    "axes[1, 1].set_xlabel('Baseline RMSE')\n",
    "axes[1, 1].set_ylabel('Soft Masking RMSE')\n",
    "axes[1, 1].set_title('Per-Series RMSE Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}