{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed40d52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Chronos-2 Soft Group Masking Extension - Progress Summary\n",
    "\n",
    "## The Idea\n",
    "\n",
    "**Hypothesis**: Hard group masking in Chronos-2 can block useful information when related time series are placed in different groups. Relaxing this constraint via similarity-weighted soft masking could allow beneficial cross-group learning while maintaining group structure.\n",
    "\n",
    "**Key insight**: Instead of binary masking (attend=1, ignore=0), use continuous weights (0 to 1) based on how similar series are, allowing the model to learn from related series even across group boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Modified Chronos-2 to add soft masking capability (inference-only, no training required):\n",
    "\n",
    "**Core changes**:\n",
    "```python\n",
    "# 1. Compute similarity between all time series in batch\n",
    "similarity_matrix = compute_input_similarity(context, similarity_type=\"correlation\")\n",
    "\n",
    "# 2. Convert similarity to soft attention mask\n",
    "# Within-group: similarity = 1.0 (full attention)\n",
    "# Cross-group: similarity = computed value (0 to 1)\n",
    "soft_mask = hard_mask + (1 - hard_mask) * similarity_matrix\n",
    "\n",
    "# 3. Apply temperature scaling and convert to attention bias\n",
    "attention_bias = log(soft_mask) × temperature\n",
    "```\n",
    "\n",
    "**Modified files**: `model.py` (soft mask construction), `pipeline.py` (similarity computation and parameter passing)\n",
    "\n",
    "---\n",
    "\n",
    "## Similarity Matrix: What We Use and Alternatives\n",
    "\n",
    "The similarity matrix determines how much cross-group attention is allowed. We implemented three approaches:\n",
    "\n",
    "### 1. **Correlation (Pearson)** - Currently Used\n",
    "- Measures linear relationships between series\n",
    "- Best for: Series with similar patterns but different scales\n",
    "- Formula: Normalized covariance\n",
    "\n",
    "### 2. **Cosine Similarity** - Available, Not Yet Tested\n",
    "- Measures directional similarity (ignores magnitude)\n",
    "- Best for: Series with similar shapes but different amplitudes\n",
    "- Formula: Angle between vectors\n",
    "\n",
    "### 3. **Distance (Gaussian Kernel)** - Available, Not Yet Tested\n",
    "- Measures numerical proximity\n",
    "- Best for: Series that are close in value space\n",
    "- Formula: exp(-euclidean_distance / scale)\n",
    "\n",
    "---\n",
    "\n",
    "## Temperature Parameter\n",
    "\n",
    "**Purpose**: Controls how permissive cross-group attention is\n",
    "\n",
    "- **Low temperature (e.g., 1.0)**: Stricter - only very similar series attend across groups (closer to hard masking)\n",
    "- **High temperature (e.g., 20.0)**: Permissive - even moderately similar series can share information\n",
    "\n",
    "**Mathematical effect**: `attention_bias = log(similarity) × temperature`\n",
    "- Acts as a scaling factor for cross-group information flow\n",
    "- Higher temperature amplifies the effect of similarity differences\n",
    "\n",
    "---\n",
    "\n",
    "## Testing Conclusions\n",
    "\n",
    "We tested the soft masking approach on multiple datasets with varying characteristics:\n",
    "\n",
    "**Key findings**:\n",
    "1. **Unrelated series (M4 Hourly)**: No improvement - as expected, cross-group learning doesn't help when series are from different domains\n",
    "\n",
    "2. **Related series (ETT datasets)**: Small improvements (~6% on full data)\n",
    "   - Statistically significant with large samples\n",
    "   - **BUT**: Predictions are 98% identical to baseline (high correlation)\n",
    "   - Only ~61% of samples improved (barely better than random)\n",
    "   - Suggests **smoothing effect** rather than meaningful cross-learning\n",
    "\n",
    "3. **Small vs large samples**: Improvement dropped from 28% (100 samples) to 6% (117k samples)\n",
    "   - Indicates overfitting on small samples\n",
    "   - Real effect is much smaller than initially observed\n",
    "\n",
    "**Current interpretation**: Soft masking provides marginal benefit in current configuration - potentially just reducing prediction variance rather than enabling meaningful cross-group learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Still to Explore\n",
    "\n",
    "To determine if soft masking can provide meaningful improvements, we need to test:\n",
    "\n",
    "1. **Different similarity measures**: \n",
    "   - Cosine similarity (shape-based)\n",
    "   - Distance-based (proximity-based)\n",
    "   - Compare which captures useful relationships best\n",
    "\n",
    "2. **Temperature sweep**: \n",
    "   - Test 1.0, 10.0, 20.0 (only tested 5.0 so far)\n",
    "   - Find optimal trade-off between group separation and cross-learning\n",
    "\n",
    "3. **Different datasets**:\n",
    "   - Weekly vs hourly granularity\n",
    "   - Datasets with stronger inter-series relationships\n",
    "\n",
    "4. **Deeper analysis**:\n",
    "   - Per-feature results (which features benefit?)\n",
    "   - Temporal patterns (does it help more for short/long-term forecasts?)\n",
    "\n",
    "**Goal**: Determine if there's a configuration where soft masking provides substantial practical benefit, or conclude it's not effective for inference-only scenarios.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71d9dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
