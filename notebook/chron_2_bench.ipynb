{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chronos-2 Soft Group Masking Test\n",
    "\n",
    "This notebook tests the soft group masking extension for Chronos-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Install modified Chronos from GitHub and dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Remove old installation if exists\n",
    "    !rm -rf /content/chronos-forecasting\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/mat0k/chronos-forecasting.git /content/chronos-forecasting\n",
    "    !cd /content/chronos-forecasting && git checkout soft_attention_2\n",
    "    \n",
    "    # Reinstall\n",
    "    !pip uninstall -y chronos\n",
    "    !pip install -e /content/chronos-forecasting\n",
    "    print(\"✓ Installed Chronos from GitHub (soft_attention_2 branch)\")\n",
    "else:\n",
    "    print(\"Not in Colab - assuming local modified version is available\")\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -q datasets gluonts pandas scipy tqdm pyyaml\n",
    "\n",
    "print(\"✓ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import datasets\n",
    "from chronos import BaseChronosPipeline\n",
    "from scipy.stats import ttest_rel\n",
    "from tqdm import tqdm\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.ev.metrics import MASE, MeanWeightedSumQuantileLoss\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model.evaluation import evaluate_forecasts\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Chronos-2 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-2\",\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded Chronos-2 model\")\n",
    "print(f\"Pipeline type: {type(pipeline).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Helper functions\n",
    "QUANTILES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "def to_gluonts_univariate(hf_dataset):\n",
    "    \"\"\"Convert HuggingFace dataset to GluonTS format\"\"\"\n",
    "    series_fields = [col for col in hf_dataset.features if isinstance(hf_dataset.features[col], datasets.Sequence)]\n",
    "    series_fields.remove(\"timestamp\")\n",
    "    dataset_length = hf_dataset.info.splits[\"train\"].num_examples * len(series_fields)\n",
    "    dataset_freq = pd.DatetimeIndex(hf_dataset[0][\"timestamp\"]).to_period()[0].freqstr\n",
    "    \n",
    "    gts_dataset = []\n",
    "    for hf_entry in hf_dataset:\n",
    "        for field in series_fields:\n",
    "            gts_dataset.append({\n",
    "                \"start\": pd.Period(hf_entry[\"timestamp\"][0], freq=dataset_freq),\n",
    "                \"target\": hf_entry[field],\n",
    "            })\n",
    "    assert len(gts_dataset) == dataset_length\n",
    "    return gts_dataset\n",
    "\n",
    "def load_and_split_dataset(backtest_config):\n",
    "    \"\"\"Load and split dataset for evaluation\"\"\"\n",
    "    hf_repo = backtest_config[\"hf_repo\"]\n",
    "    dataset_name = backtest_config[\"name\"]\n",
    "    offset = backtest_config[\"offset\"]\n",
    "    prediction_length = backtest_config[\"prediction_length\"]\n",
    "    num_rolls = backtest_config[\"num_rolls\"]\n",
    "    \n",
    "    trust_remote_code = True if hf_repo == \"autogluon/chronos_datasets_extra\" else False\n",
    "    ds = datasets.load_dataset(hf_repo, dataset_name, split=\"train\", trust_remote_code=trust_remote_code)\n",
    "    ds.set_format(\"numpy\")\n",
    "    \n",
    "    gts_dataset = to_gluonts_univariate(ds)\n",
    "    _, test_template = split(gts_dataset, offset=offset)\n",
    "    test_data = test_template.generate_instances(prediction_length, windows=num_rolls)\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "def generate_forecasts(test_data_input, pipeline, prediction_length, batch_size, **predict_kwargs):\n",
    "    \"\"\"Generate forecasts using pipeline\"\"\"\n",
    "    forecast_outputs = []\n",
    "    for batch in tqdm(batcher(test_data_input, batch_size=batch_size), desc=\"Generating forecasts\"):\n",
    "        context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "        quantiles, _ = pipeline.predict_quantiles(\n",
    "            context,\n",
    "            prediction_length=prediction_length,\n",
    "            quantile_levels=QUANTILES,\n",
    "            **predict_kwargs,\n",
    "        )\n",
    "        if isinstance(quantiles, list):\n",
    "            quantiles = np.stack(quantiles).squeeze(axis=1)\n",
    "        quantiles = quantiles.swapaxes(-1, -2)\n",
    "        forecast_outputs.append(quantiles)\n",
    "    forecast_outputs = np.concatenate(forecast_outputs)\n",
    "    \n",
    "    # Convert to gluonts QuantileForecast objects\n",
    "    forecasts = []\n",
    "    for item, ts in zip(forecast_outputs, test_data_input):\n",
    "        forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "        forecasts.append(\n",
    "            QuantileForecast(\n",
    "                forecast_arrays=item,\n",
    "                forecast_keys=list(map(str, QUANTILES)),\n",
    "                start_date=forecast_start_date,\n",
    "            )\n",
    "        )\n",
    "    return forecasts\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load benchmark configuration\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # In Colab, use absolute path\n",
    "    config_path = \"/content/chronos-forecasting/scripts/evaluation/configs/zero-shot.yaml\"\n",
    "else:\n",
    "    # Local environment, use relative path\n",
    "    config_path = \"../scripts/evaluation/configs/zero-shot.yaml\"\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(config_path):\n",
    "    print(f\"ERROR: Config file not found at: {config_path}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    raise FileNotFoundError(f\"Cannot find {config_path}\")\n",
    "\n",
    "# Load zero-shot benchmark configs\n",
    "with open(config_path) as fp:\n",
    "    backtest_configs = yaml.safe_load(fp)\n",
    "\n",
    "print(f\"✓ Loaded {len(backtest_configs)} dataset configurations\")\n",
    "print(\"\\nDatasets to test:\")\n",
    "for i, config in enumerate(backtest_configs, 1):\n",
    "    print(f\"{i:2d}. {config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run benchmark on all datasets\n",
    "batch_size = 32\n",
    "results = []\n",
    "\n",
    "for config_idx, config in enumerate(backtest_configs, 1):\n",
    "    dataset_name = config[\"name\"]\n",
    "    prediction_length = config[\"prediction_length\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{config_idx}/{len(backtest_configs)}] Processing: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"Loading {dataset_name}...\")\n",
    "    test_data = load_and_split_dataset(backtest_config=config)\n",
    "    print(f\"✓ Loaded {len(test_data.input)} time series\")\n",
    "    \n",
    "    # BASELINE: Generate forecasts with hard masking\n",
    "    print(f\"\\nBASELINE (Hard Group Masking):\")\n",
    "    baseline_forecasts = generate_forecasts(\n",
    "        test_data.input,\n",
    "        pipeline=pipeline,\n",
    "        prediction_length=prediction_length,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    # Evaluate baseline\n",
    "    baseline_metrics = evaluate_forecasts(\n",
    "        baseline_forecasts,\n",
    "        test_data=test_data,\n",
    "        metrics=[MASE(), MeanWeightedSumQuantileLoss(QUANTILES)],\n",
    "        batch_size=5000,\n",
    "    ).reset_index(drop=True).to_dict(orient=\"records\")[0]\n",
    "    \n",
    "    baseline_mase = baseline_metrics[\"MASE[0.5]\"]\n",
    "    baseline_wql = baseline_metrics[\"mean_weighted_sum_quantile_loss\"]\n",
    "    \n",
    "    print(f\"  MASE: {baseline_mase:.4f}\")\n",
    "    print(f\"  WQL:  {baseline_wql:.4f}\")\n",
    "    \n",
    "    # SOFT MASKING: Generate forecasts with soft masking\n",
    "    print(f\"\\nSOFT MASKING (Correlation-based, temperature=5.0):\")\n",
    "    soft_forecasts = generate_forecasts(\n",
    "        test_data.input,\n",
    "        pipeline=pipeline,\n",
    "        prediction_length=prediction_length,\n",
    "        batch_size=batch_size,\n",
    "        use_soft_group_mask=True,\n",
    "        similarity_type=\"correlation\",\n",
    "        soft_mask_temperature=5.0,\n",
    "    )\n",
    "    \n",
    "    # Evaluate soft masking\n",
    "    soft_metrics = evaluate_forecasts(\n",
    "        soft_forecasts,\n",
    "        test_data=test_data,\n",
    "        metrics=[MASE(), MeanWeightedSumQuantileLoss(QUANTILES)],\n",
    "        batch_size=5000,\n",
    "    ).reset_index(drop=True).to_dict(orient=\"records\")[0]\n",
    "    \n",
    "    soft_mase = soft_metrics[\"MASE[0.5]\"]\n",
    "    soft_wql = soft_metrics[\"mean_weighted_sum_quantile_loss\"]\n",
    "    \n",
    "    print(f\"  MASE: {soft_mase:.4f}\")\n",
    "    print(f\"  WQL:  {soft_wql:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"baseline_mase\": baseline_mase,\n",
    "        \"baseline_wql\": baseline_wql,\n",
    "        \"soft_mase\": soft_mase,\n",
    "        \"soft_wql\": soft_wql,\n",
    "    })\n",
    "    \n",
    "    # Print improvement\n",
    "    mase_improvement = ((baseline_mase - soft_mase) / baseline_mase) * 100\n",
    "    wql_improvement = ((baseline_wql - soft_wql) / baseline_wql) * 100\n",
    "    print(f\"\\nImprovement:\")\n",
    "    print(f\"  MASE: {mase_improvement:+.2f}%\")\n",
    "    print(f\"  WQL:  {wql_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ All datasets processed\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Summary and statistical analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_mase_mean = results_df[\"baseline_mase\"].mean()\n",
    "soft_mase_mean = results_df[\"soft_mase\"].mean()\n",
    "baseline_wql_mean = results_df[\"baseline_wql\"].mean()\n",
    "soft_wql_mean = results_df[\"soft_wql\"].mean()\n",
    "\n",
    "print(f\"\\nBASELINE (Hard Group Masking):\")\n",
    "print(f\"  Average MASE: {baseline_mase_mean:.4f}\")\n",
    "print(f\"  Average WQL:  {baseline_wql_mean:.4f}\")\n",
    "\n",
    "print(f\"\\nSOFT MASKING (Correlation-based):\")\n",
    "print(f\"  Average MASE: {soft_mase_mean:.4f}\")\n",
    "print(f\"  Average WQL:  {soft_wql_mean:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "mase_improvement = ((baseline_mase_mean - soft_mase_mean) / baseline_mase_mean) * 100\n",
    "wql_improvement = ((baseline_wql_mean - soft_wql_mean) / baseline_wql_mean) * 100\n",
    "\n",
    "print(f\"\\nIMPROVEMENT:\")\n",
    "print(f\"  MASE: {mase_improvement:+.2f}%\")\n",
    "print(f\"  WQL:  {wql_improvement:+.2f}%\")\n",
    "\n",
    "# Paired t-test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE (Paired t-test)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mase_t_stat, mase_p_value = ttest_rel(\n",
    "    results_df[\"baseline_mase\"], \n",
    "    results_df[\"soft_mase\"]\n",
    ")\n",
    "wql_t_stat, wql_p_value = ttest_rel(\n",
    "    results_df[\"baseline_wql\"], \n",
    "    results_df[\"soft_wql\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nMASE: t={mase_t_stat:.4f}, p={mase_p_value:.6f}\")\n",
    "print(f\"WQL:  t={wql_t_stat:.4f}, p={wql_p_value:.6f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance level: α = {alpha}\")\n",
    "\n",
    "if mase_p_value < alpha:\n",
    "    print(f\"✓ MASE difference is statistically significant (p < {alpha})\")\n",
    "else:\n",
    "    print(f\"✗ MASE difference is NOT statistically significant (p >= {alpha})\")\n",
    "\n",
    "if wql_p_value < alpha:\n",
    "    print(f\"✓ WQL difference is statistically significant (p < {alpha})\")\n",
    "else:\n",
    "    print(f\"✗ WQL difference is NOT statistically significant (p >= {alpha})\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECT SIZE (Cohen's d)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pooled_std_mase = np.sqrt(\n",
    "    (results_df[\"baseline_mase\"].std()**2 + results_df[\"soft_mase\"].std()**2) / 2\n",
    ")\n",
    "cohens_d_mase = (baseline_mase_mean - soft_mase_mean) / pooled_std_mase\n",
    "\n",
    "pooled_std_wql = np.sqrt(\n",
    "    (results_df[\"baseline_wql\"].std()**2 + results_df[\"soft_wql\"].std()**2) / 2\n",
    ")\n",
    "cohens_d_wql = (baseline_wql_mean - soft_wql_mean) / pooled_std_wql\n",
    "\n",
    "print(f\"\\nMASE Cohen's d: {cohens_d_mase:.4f}\")\n",
    "print(f\"WQL Cohen's d:  {cohens_d_wql:.4f}\")\n",
    "print(\"\\nInterpretation: <0.2=negligible, 0.2-0.5=small, 0.5-0.8=medium, >0.8=large\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
